{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEr1Z61hFTGnLuLGzoPZ4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megumihoshino/ML-intermediate_new/blob/main/Batch_Loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0PMzTrrK_hl"
      },
      "outputs": [],
      "source": [
        "#BATCH LOADING\n",
        "'''\n",
        "- Batch Loading: proses pelatihan ketika NN melakukan pembaruan parameternya(weight)\n",
        "stlh membaca sejumlh sampel data tertentu\n",
        "- Hyperparameter: parameter yg tdk ditentukan oleh model itu sendiri\n",
        "slm proses pembljran, ttpi hrs ditentukan oleh dev atau pembuat model\n",
        "sblm pelatihan dimulai. dgn kt lain, variabel yg mengontrol proses\n",
        "pembeljran dan arsitektur model, sprt kec. pembljran, jml layer dan neuron dlm\n",
        "NN, ukuran batch, dll.\n",
        "\n",
        "cthnya:\n",
        "ketika kita pny dataset sejml 800 buah, maka jika tnp batch size, maka 1\n",
        "kali epoch akn terdpt 800 kli pembaruan weight.\n",
        "nah, ketika kita atur 1 batch adlh 32 gbr, maka akan terdpt 25 batch\n",
        "pd dataset. shg pembaruan parameter pd 1 epoch hny sebyk 25 kli.\n",
        "\n",
        "TUJUAN: utk membuat proses pelatihan data mjd lbh cpt\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPLEMENTASI BATCH LOADING\n",
        "\n",
        "import tensorflow as tf\n",
        "mnist= tf.keras.datasets.mnist\n",
        "(image_train, labels_train), (image_test, labels_test) = mnist.load_data()\n",
        "\n",
        "image_train = image_train/255.0\n",
        "image_test = image_test/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape = (28,28)),\n",
        "    tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation = tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(image_train, labels_train, batch_size = 128, epochs= 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cUj_ot2QIqa",
        "outputId": "a1119d67-4b54-445c-d323-5d1c9c99a101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8323 - loss: 0.6034\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9484 - loss: 0.1832\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9645 - loss: 0.1272\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9735 - loss: 0.0956\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9784 - loss: 0.0738\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f6336cd29e0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DROPOUT N BATCH NORMALIZATION UTK MENCEGAH OVERFITTING\n",
        "\n",
        "from keras.layers import Dense, Dropout\n",
        "model = Sequential([\n",
        "                    Dense(64, activation='relu', input_shape=(4,)),\n",
        "                    Dense(64, activation='relu'),\n",
        "                    Dropout(0,5),\n",
        "                    Dense(3, activation='softmax')])\n",
        "\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "model = Sequential([\n",
        "                    Dense(64, activation='relu', input_shape=(4,)),\n",
        "                    Dense(64, activation='relu'),\n",
        "                    Dropout(0,5),\n",
        "                    BatchNormalization(momentum=0.99),\n",
        "                    Dense(3, activation='softmax')])"
      ],
      "metadata": {
        "id": "qFJ_S5TAULKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIMASI TRAINING DG FUNGSI CALLBACKS\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "df = pd.read_csv('https://drive.google.com/uc?id=1roJ83AbgzDcvRr0Gwud0BmdUQx-oSG-w')\n",
        "df = df.drop(columns='Id')\n",
        "category = pd.get_dummies(df.Species, dtype=int)\n",
        "new_df = pd.concat([df, category], axis=1)\n",
        "new_df = new_df.drop(columns='Species')\n",
        "dataset = new_df.values\n",
        "# Pilih 4 kolom pertama untuk dijadikan sebagai atribut\n",
        "X = dataset[:,0:4]\n",
        "# Pilih 3 kolom terakhir sebagai label\n",
        "y = dataset[:,4:7]\n",
        "# Normalisasi\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(X)\n",
        "X_scale\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scale, y, test_size=0.3)\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "model = Sequential([\n",
        "                    Dense(64, activation='relu', input_shape=(4,)),\n",
        "                    Dense(64, activation='relu'),\n",
        "                    Dropout(0,5),\n",
        "                    BatchNormalization(momentum=0.99),\n",
        "                    Dense(3, activation='softmax')])\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "hist = model.fit(X_train, Y_train, epochs=100)\n"
      ],
      "metadata": {
        "id": "5rxNniDpUeag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.9):\n",
        "      print(\"\\nAkurasi telah mencapai >90%!\")\n",
        "      self.model.stop_training = True\n",
        "callbacks = myCallback()\n",
        "\n",
        "hist = model.fit(X_train, Y_train, epochs=100, callbacks = [callbacks])\n"
      ],
      "metadata": {
        "id": "RmPQJaAeUqjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}